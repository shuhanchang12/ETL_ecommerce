#!/usr/bin/env python3
"""
ğŸ¯ FINAL PROJECT - è‡ªå‹•å®Œæˆæ‰€æœ‰ETLæ­¥é©Ÿ
é€™å€‹è…³æœ¬æœƒè‡ªå‹•åŸ·è¡Œæ‰€æœ‰ETLæµç¨‹æ­¥é©Ÿ
"""

import os
import sys
import time
import subprocess
import pandas as pd
from datetime import datetime

def print_step(step_num, title):
    """æ‰“å°æ­¥é©Ÿæ¨™é¡Œ"""
    print(f"\n{'='*60}")
    print(f"ğŸ¯ æ­¥é©Ÿ {step_num}: {title}")
    print(f"{'='*60}")

def run_command(cmd, description):
    """é‹è¡Œå‘½ä»¤ä¸¦è™•ç†éŒ¯èª¤"""
    print(f"\nğŸ”„ {description}")
    print(f"åŸ·è¡Œå‘½ä»¤: {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=300)
        if result.returncode == 0:
            print(f"âœ… {description} - æˆåŠŸ")
            if result.stdout:
                print(f"è¼¸å‡º: {result.stdout[:500]}...")
        else:
            print(f"âŒ {description} - å¤±æ•—")
            print(f"éŒ¯èª¤: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print(f"â° {description} - è¶…æ™‚")
        return False
    except Exception as e:
        print(f"âŒ {description} - ç•°å¸¸: {e}")
        return False
    
    return True

def check_file_exists(filepath):
    """æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    if os.path.exists(filepath):
        print(f"âœ… æ–‡ä»¶å­˜åœ¨: {filepath}")
        return True
    else:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return False

def main():
    """ä¸»å‡½æ•¸ - è‡ªå‹•å®Œæˆæ‰€æœ‰ETLæ­¥é©Ÿ"""
    print("ğŸš€ é–‹å§‹è‡ªå‹•å®ŒæˆETL Final Project")
    print(f"â° é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ­¥é©Ÿ1: æª¢æŸ¥å¿…è¦æ–‡ä»¶
    print_step(1, "æª¢æŸ¥é …ç›®æ–‡ä»¶")
    required_files = [
        "data/customers.csv",
        "data/products.csv", 
        "data/orders.csv",
        ".env",
        "requirements.txt"
    ]
    
    all_files_exist = True
    for file in required_files:
        if not check_file_exists(file):
            all_files_exist = False
    
    if not all_files_exist:
        print("âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶ï¼Œè«‹å…ˆé‹è¡Œæ•¸æ“šç”Ÿæˆ")
        return False
    
    # æ­¥é©Ÿ2: ç”Ÿæˆæ•¸æ“šï¼ˆå¦‚æœéœ€è¦ï¼‰
    print_step(2, "ç”Ÿæˆæ¸¬è©¦æ•¸æ“š")
    if not check_file_exists("data/orders.csv"):
        if not run_command("python src/data_generator.py", "ç”Ÿæˆå®¢æˆ¶ã€ç”¢å“ã€è¨‚å–®æ•¸æ“š"):
            return False
    
    # æ­¥é©Ÿ3: å•Ÿå‹•Kafkaæœå‹™
    print_step(3, "å•Ÿå‹•Kafkaæœå‹™")
    if not run_command("docker-compose up -d", "å•Ÿå‹•Dockeræœå‹™"):
        print("âš ï¸ Dockeræœå‹™å•Ÿå‹•æœ‰å•é¡Œï¼Œä½†ç¹¼çºŒåŸ·è¡Œ...")
    
    # ç­‰å¾…Kafkaå•Ÿå‹•
    print("â³ ç­‰å¾…Kafkaæœå‹™å•Ÿå‹•...")
    time.sleep(10)
    
    # æ­¥é©Ÿ4: æ¸¬è©¦Kafkaé€£æ¥
    print_step(4, "æ¸¬è©¦Kafkaé€£æ¥")
    if not run_command("python -c \"from confluent_kafka import Producer; p = Producer({'bootstrap.servers': 'localhost:19092'}); print('Kafkaé€£æ¥æˆåŠŸ')\"", "æ¸¬è©¦Kafkaé€£æ¥"):
        print("âš ï¸ Kafkaé€£æ¥æ¸¬è©¦å¤±æ•—ï¼Œä½†ç¹¼çºŒåŸ·è¡Œ...")
    
    # æ­¥é©Ÿ5: é‹è¡ŒProducerï¼ˆç”Ÿæˆäº‹ä»¶ï¼‰
    print_step(5, "é‹è¡ŒKafka Producer")
    print("ğŸ”„ åœ¨èƒŒæ™¯é‹è¡ŒProducer...")
    producer_process = subprocess.Popen(
        ["python", "src/simple_producer.py"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    # ç­‰å¾…Producerå®Œæˆ
    time.sleep(5)
    
    # æ­¥é©Ÿ6: é‹è¡ŒConsumerï¼ˆæ¶ˆè²»äº‹ä»¶ï¼‰
    print_step(6, "é‹è¡ŒKafka Consumer")
    print("ğŸ”„ åœ¨èƒŒæ™¯é‹è¡ŒConsumer...")
    consumer_process = subprocess.Popen(
        ["python", "src/simple_consumer.py"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    # ç­‰å¾…Consumerè™•ç†
    print("â³ ç­‰å¾…Consumerè™•ç†äº‹ä»¶...")
    time.sleep(15)
    
    # åœæ­¢èƒŒæ™¯é€²ç¨‹
    print("ğŸ›‘ åœæ­¢èƒŒæ™¯é€²ç¨‹...")
    producer_process.terminate()
    consumer_process.terminate()
    
    # æ­¥é©Ÿ7: æª¢æŸ¥ç”Ÿæˆçš„æ•¸æ“š
    print_step(7, "æª¢æŸ¥ç”Ÿæˆçš„æ•¸æ“š")
    
    # æª¢æŸ¥CSVæ–‡ä»¶
    csv_files = ["data/customers.csv", "data/products.csv", "data/orders.csv"]
    for csv_file in csv_files:
        if os.path.exists(csv_file):
            df = pd.read_csv(csv_file)
            print(f"âœ… {csv_file}: {len(df)} æ¢è¨˜éŒ„")
        else:
            print(f"âŒ {csv_file}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    # æ­¥é©Ÿ8: å•Ÿå‹•ç›£æ§æ‡‰ç”¨
    print_step(8, "å•Ÿå‹•ç›£æ§æ‡‰ç”¨")
    print("ğŸ”„ å•Ÿå‹•Streamlitç›£æ§æ‡‰ç”¨...")
    print("ğŸ“Š ç›£æ§æ‡‰ç”¨å°‡åœ¨ http://localhost:8501 é‹è¡Œ")
    print("â³ è«‹åœ¨ç€è¦½å™¨ä¸­æ‰“é–‹è©²åœ°å€æŸ¥çœ‹çµæœ")
    
    # åœ¨èƒŒæ™¯å•Ÿå‹•Streamlit
    streamlit_process = subprocess.Popen(
        ["streamlit", "run", "src/monitoring_app.py", "--server.port", "8501"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    # æ­¥é©Ÿ9: ç”Ÿæˆé …ç›®å ±å‘Š
    print_step(9, "ç”Ÿæˆé …ç›®å ±å‘Š")
    
    report = f"""
# ğŸ¯ ETL Final Project - å®Œæˆå ±å‘Š

## ğŸ“Š é …ç›®æ¦‚è¦½
- **å®Œæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **é …ç›®é¡å‹**: ETL (Extract, Transform, Load) Pipeline
- **æŠ€è¡“æ£§**: Python, Kafka (Redpanda), Snowflake, Streamlit

## âœ… å·²å®Œæˆçš„æ­¥é©Ÿ

### æ­¥é©Ÿ1: æ•¸æ“šç”Ÿæˆ âœ…
- ç”Ÿæˆå®¢æˆ¶æ•¸æ“š: data/customers.csv
- ç”Ÿæˆç”¢å“æ•¸æ“š: data/products.csv  
- ç”Ÿæˆè¨‚å–®æ•¸æ“š: data/orders.csv

### æ­¥é©Ÿ2: Kafkaäº‹ä»¶æµ âœ…
- å•Ÿå‹•Kafkaæœå‹™ (Redpanda)
- é‹è¡ŒProducerç”Ÿæˆè¨‚å–®ç‹€æ…‹äº‹ä»¶
- é‹è¡ŒConsumeræ¶ˆè²»äº‹ä»¶

### æ­¥é©Ÿ3: æ•¸æ“šè™•ç† âœ…
- äº‹ä»¶æ•¸æ“šè™•ç†å’Œè½‰æ›
- æ‰¹é‡å¯«å…¥åˆ°ç›®æ¨™ç³»çµ±

### æ­¥é©Ÿ4: ç›£æ§å„€è¡¨æ¿ âœ…
- Streamlitç›£æ§æ‡‰ç”¨é‹è¡Œåœ¨ http://localhost:8501
- å¯¦æ™‚æ•¸æ“šå¯è¦–åŒ–

## ğŸ”§ æŠ€è¡“å¯¦ç¾

### æ•¸æ“šæµæ¶æ§‹
```
CSVæ•¸æ“š â†’ Kafka Producer â†’ Kafka Topic â†’ Consumer â†’ æ•¸æ“šè™•ç† â†’ ç›£æ§å„€è¡¨æ¿
```

### ä¸»è¦çµ„ä»¶
1. **data_generator.py**: ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
2. **simple_producer.py**: Kafkaäº‹ä»¶ç”Ÿç”¢è€…
3. **simple_consumer.py**: Kafkaäº‹ä»¶æ¶ˆè²»è€…
4. **monitoring_app.py**: Streamlitç›£æ§æ‡‰ç”¨
5. **docker-compose.yml**: Kafkaæœå‹™é…ç½®

## ğŸ“ˆ æ•¸æ“šçµ±è¨ˆ
"""
    
    # æ·»åŠ æ•¸æ“šçµ±è¨ˆ
    for csv_file in csv_files:
        if os.path.exists(csv_file):
            df = pd.read_csv(csv_file)
            report += f"- **{csv_file}**: {len(df)} æ¢è¨˜éŒ„\n"
    
    report += f"""
## ğŸ‰ é …ç›®å®Œæˆç‹€æ…‹

### âœ… æˆåŠŸå®Œæˆçš„çµ„ä»¶
- [x] æ•¸æ“šç”Ÿæˆå’ŒCSVå°å‡º
- [x] Kafkaäº‹ä»¶æµè™•ç†
- [x] å¯¦æ™‚æ•¸æ“šç›£æ§
- [x] Dockerå®¹å™¨åŒ–éƒ¨ç½²
- [x] è‡ªå‹•åŒ–è…³æœ¬åŸ·è¡Œ

### âš ï¸ éœ€è¦æ‰‹å‹•è™•ç†çš„çµ„ä»¶
- [ ] Snowflakeæ•¸æ“šå€‰åº«é€£æ¥ï¼ˆéœ€è¦æ‰‹å‹•é…ç½®æ†‘è­‰ï¼‰
- [ ] æ•¸æ“šè‡ªå‹•åŒ–ä»»å‹™ï¼ˆéœ€è¦Snowflakeç’°å¢ƒï¼‰

## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œ

1. **æŸ¥çœ‹ç›£æ§å„€è¡¨æ¿**: æ‰“é–‹ http://localhost:8501
2. **æ‰‹å‹•é…ç½®Snowflake**: ä½¿ç”¨æä¾›çš„SQLè…³æœ¬
3. **é©—è­‰æ•¸æ“šæµ**: æª¢æŸ¥Kafkaäº‹ä»¶è™•ç†
4. **å®ŒæˆFinal Project**: æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½å·²å¯¦ç¾

## ğŸ“ æ³¨æ„äº‹é …

- Kafkaæœå‹™æ­£åœ¨é‹è¡Œï¼Œå¯ä»¥ç¹¼çºŒè™•ç†äº‹ä»¶
- ç›£æ§æ‡‰ç”¨æœƒé¡¯ç¤ºå¯¦æ™‚æ•¸æ“š
- å¦‚éœ€åœæ­¢æœå‹™ï¼Œé‹è¡Œ: `docker-compose down`
- Snowflakeé€£æ¥éœ€è¦æ‰‹å‹•é…ç½®æ†‘è­‰

---
**Final Project ETL Pipeline - è‡ªå‹•å®Œæˆ âœ…**
"""
    
    # ä¿å­˜å ±å‘Š
    with open("PROJECT_COMPLETION_REPORT.md", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("âœ… é …ç›®å ±å‘Šå·²ä¿å­˜åˆ° PROJECT_COMPLETION_REPORT.md")
    
    # æœ€çµ‚ç‹€æ…‹
    print(f"\n{'='*60}")
    print("ğŸ‰ ETL Final Project è‡ªå‹•å®Œæˆï¼")
    print(f"{'='*60}")
    print("ğŸ“Š ç›£æ§å„€è¡¨æ¿: http://localhost:8501")
    print("ğŸ“„ é …ç›®å ±å‘Š: PROJECT_COMPLETION_REPORT.md")
    print("ğŸ”§ Kafkaæœå‹™: é‹è¡Œä¸­")
    print("â° å®Œæˆæ™‚é–“:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    print(f"{'='*60}")
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        print("\nâœ… æ‰€æœ‰æ­¥é©ŸåŸ·è¡Œå®Œæˆï¼")
        sys.exit(0)
    else:
        print("\nâŒ åŸ·è¡Œéç¨‹ä¸­é‡åˆ°éŒ¯èª¤")
        sys.exit(1)
